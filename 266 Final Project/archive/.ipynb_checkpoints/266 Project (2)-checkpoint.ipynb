{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mariam/Desktop/266 Final Project'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + \"/266 Photos/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "\n",
    "    all_files = glob.glob(path + \"/*.png\")\n",
    "    sentences = []\n",
    "    images = []\n",
    "    \n",
    "    for filename in all_files:\n",
    "        img = Image.open(filename)\n",
    "        \n",
    "        sentence = os.path.basename(os.path.normpath(os.path.splitext(filename)[0]))\n",
    "        sentences.append(sentence)\n",
    "\n",
    "        images.append(img)\n",
    "\n",
    "    return sentences, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, images = read_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_characters(img):\n",
    "    \n",
    "    \"\"\"Splits an image of a Braille sentence by character. Returns a list of list of numpy arrays\"\"\"\n",
    "    \n",
    "    sums = np.sum(np.array(img), axis=0)\n",
    "    \n",
    "    ids = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    i = 0\n",
    "    while i < len(sums):\n",
    "        if sums[start] == 28305:\n",
    "            if (sums[end] != 28305) or (end == (len(sums)-1)):\n",
    "                if end - start > 2:\n",
    "                    ids.append((start, end))\n",
    "                start = end\n",
    "        else:        \n",
    "            start += 1\n",
    "        end += 1\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    mp = [int(np.round(np.mean(i))) for i in ids]\n",
    "    \n",
    "    chars = []\n",
    "    while i < len(mp)-1:\n",
    "        chars.append((np.array(img).T[mp[i]:mp[i+1]]).T)\n",
    "        i += 1\n",
    "        \n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [split_characters(img) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82779b2650>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAAD7CAYAAACCGwFxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJpElEQVR4nO3da4hc5R3H8e+vu4pVa3PR2G0STVpCLxRaJcTUFimmgqal8YWCUlqVQCjYGi9FU9/4qqBFvPSFQtC0looaomAooRLS2AtijKbWS1btsqW6zdaN1agorY3+++I8adbNbDa758zu/J3fB2R2zp6Z88DXc2Z29HlGEYF1vo/N9ADsyDhUEg6VhEMl4VBJOFQSbQkl6VxJL0oakLSuHcfoNmr67yhJPcBLwDnAELATuDgidjd6oC7T24bnXAYMRMQggKT7gVXAuKFOnNMTixYe1Yah5PPUM/95LSJOGru9HaHmA6+Muj8EnDF2J0lrgDUAp8zv5YlHFrZhKPn09A38vdX2drxGqcW2Q66vEbE+IpZGxNKT5va0YRgfLe0INQSMPj0WAHvacJyu0o5QO4ElkhZLOhq4CNjchuN0lcZfoyJiv6QfAo8APcCGiHi+6eN0m3a8mSAitgBb2vHc3cqfTCThUEk4VBIOlYRDJeFQSThUEg6VhEMl4VBJOFQSDpWEQyXhUEk4VBIOlYRDJeFQSThUEg6VhEMl4VBJOFQSDpWEQyXhUEk4VBIOlYRDJeFQSUw5lKSFkrZL6pf0vKS1ZfscSVsl/bXczm5uuN2rzhm1H7gmIr4ALAcul/RFYB2wLSKWANvKfatpyqEiYjgidpWf3wb6qWbErwLuKbvdA5xfd5DW0GuUpEXAacAO4OSIGIYqJjCviWN0u9qhJB0PPAhcGRFvTeJxayQ9KenJvf96v+4wPvJqhZJ0FFWkeyPiobL5VUl95fd9wEirx3qdicmp865PwN1Af0TcMupXm4FLys+XAA9PfXh2QJ1Z8V8Dvgc8K+npsu164EZgo6TVwMvAhfWGaFAjVET8idbL6QCsmOrzWmv+ZCIJh0rCoZJwqCQcKgmHSsKhknCoJBwqCYdKwqGScKgkHCoJh0rCoZJwqCQcKgmHSsKhknCoJBwqCYdKwqGScKgkHCoJh0rCoZJwqCQcKgmHSsKhkmhiDm+PpD9L+k25v1jSjrLOxAPlS5OtpibOqLVUSxcccBNwa1ln4g1gdQPH6Hp1J1svAL4F3FXuCzgb2FR28ToTDal7Rt0GXAt8UO7PBfZFxP5yf4hqkZBDePmCyakzK/7bwEhEPDV6c4tdo9XjvXzB5NSdFf8dSSuBY4ATqM6wWZJ6y1m1ANhTf5hWZy2kn0TEgohYBFwE/C4ivgtsBy4ou3mdiYa04++o64CrJQ1QvWbd3YZjdJ06l77/i4hHgUfLz4PAsiae1w7yJxNJOFQSDpWEQyXhUEk4VBIOlYRDJeFQSThUEg6VhEMl4VBJOFQSDpWEQyXhUEk4VBIOlYRDJeFQSThUEg6VhEMl4VBJOFQSDpWEQyXhUEk4VBJ1J1vPkrRJ0guS+iV9VdIcSVvL8gVbJc1uarDdrO4ZdTvw24j4PPBlqmUM1gHbyvIF28p9q6nOZOsTgLMoMwoj4r2I2Aesolq2ALx8QWPqnFGfAfYCvygrt9wl6Tjg5IgYBii381o92MsXTE6dUL3A6cCdEXEa8A6TuMx5+YLJqRNqCBiKiB3l/iaqcK9K6gMotyP1hmhQb/mCfwKvSPpc2bQC2A1splq2ALx8QWPqzor/EXBvWUFsELiMKv5GSauBl4ELax7DqBkqIp4Glrb41Yo6z2uH8icTSThUEg6VhEMl4VBJOFQSDpWEQyXhUEk4VBIOlUQjS5V2szfefxeAM359DQCfvX8fAPtnHQPAyFX/BuAvy+6rdRyHqums238MwOKbHwMOrtR/4FL1qd9Xt8u3VAtXP/6VTUyFL31J+IyaopH33wFg/m1PAON8XcIoR6+fU/1wx9SO5zMqCZ9RU/ReVOdQfDDRuVTR/on3ORyfUUn4jJqiBb3HAzDygzMAmHfHY4fdf++l79Y6ns+oJHxG1fTwup8BsPKEawE49Z5BAGLOJwF44drjABg8c0Ot4/iMSsJnVE2nlNeq564ofyBd0Z7j+IxKwqGScKgkHCoJh0rCoZJwqCTqLl9wlaTnJT0n6T5Jx0haLGlHWb7ggTJ3ymqqMyt+PtWfd0sj4ktAD9UXJ98E3FqWL3gDWN3EQLtd3UtfL/BxSb3AscAwcDbVfF7w8gWNqTOH9x/AzVTTP4eBN4GngH3le+KhmpA9v9XjvXzB5NS59M2mWvxjMfBp4DjgvBa7tvxPoF6+YHLqXPq+CfwtIvZGxH+Bh4AzgVnlUgiwANhTc4xGvVAvA8slHStJHFy+YDtwQdnHyxc0pM5r1A6qNw27gGfLc60HrgOuljQAzKWslWT11F2+4AbghjGbB4FldZ7XDuVPJpJwqCQcKgmHSsKhknCoJBwqCYdKwqGScKgkHCoJh0rCoZJwqCQcKgmHSsKhknCoJBwqCYdKwqGScKgkHCoJh0rCoZJwqCQcKgmHSsKhknCoJCYMJWmDpBFJz43aNkfS1rJEwdYyTRRVfi5pQNIzkk5v5+C7yZGcUb8Ezh2zbR2wrSxRsI2DXz1+HrCk/LMGuLOZYdqEoSLiD8DrYzavolqaAD68RMEq4FdReZxqPm9fU4PtZlN9jTo5IoYByu28sn0+8Mqo/bx8QUOafjOhFtu8fEEDphrq1QOXtHI7UrYPAQtH7eflCxoy1VCbqZYmgA8vUbAZ+H5597ccePPAJdLqmXBWvKT7gG8AJ0oaopoFfyOwUdJqqvUmLiy7bwFWAgPAu8BlbRhzV5owVERcPM6vVrTYN4DL6w7KDuVPJpJwqCQcKgmHSsKhknCoJBwqCYdKwqGScKgkHCoJh0rCoZJwqCQcKglFHNmX0rd1ENJe4B3gtZkeyzhOZPrGdmpEnDR2Y0eEApD0ZEQsnelxtNIJY/OlLwmHSqKTQq2f6QEcxoyPrWNeo+zwOumMssNwqCQ6IpSkcyW9WOZVrZv4EW0dy0JJ2yX1l6+uXVu2t5wTNm3jmunXKEk9wEvAOVT/7/pO4OKI2D1D4+kD+iJil6RPUH3B5vnApcDrEXFj+ZdpdkRcN13j6oQzahkwEBGDEfEecD/VPKsZERHDEbGr/Pw20E81dWi8OWHTohNCHfGcqukmaRFwGrCD8eeETYtOCHXEc6qmk6TjgQeBKyPirZkeTyeE6rg5VZKOoop0b0Q8VDaPNydsWnRCqJ3AEkmLJR1N9X3zm2dqMOWrau8G+iPillG/Gm9O2PSMa6bf9QFIWgncBvQAGyLipzM4lq8Df6T6ytoPyubrqV6nNgKnUOaERcTYSejtG1cnhLKJdcKlz46AQyXhUEk4VBIOlYRDJeFQSfwP3JqW9PLosEQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#first char of first sentence\n",
    "plt.imshow(chars[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am happy'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize list order\n",
    "import random\n",
    "z = list(zip(images, sentences))\n",
    "random.shuffle(z)\n",
    "images, sentences = zip(*z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images = np.array([np.array(images[i]).flatten() for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some variables to hold test, dev, and training data.\n",
    "train_data, train_labels = new_images[:-18], sentences[:-18]\n",
    "test_data, test_labels = new_images[-6:], sentences[-6:]\n",
    "dev_data, dev_labels = new_images[-18:-6], sentences[-18:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(3)\n",
    "trained_knn = knn.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predict = trained_knn.predict(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Every child likes ice cream', 'Every child likes ice cream',\n",
       "       'California is in America', 'I attend UC Berkeley',\n",
       "       'Every child likes ice cream', 'I like to eat apples',\n",
       "       'Go do your homework', 'Every child likes ice cream',\n",
       "       'Hi my name is Anna', 'I attend UC Berkeley', 'I went to the mall',\n",
       "       'My favorite color is pink'], dtype='<U39')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I am happy',\n",
       " 'I am behind in school work',\n",
       " 'Christina wears a red shirt',\n",
       " 'I like to play video games',\n",
       " 'She is 12 years old',\n",
       " 'My name is Mariam',\n",
       " 'Today is a good day',\n",
       " 'I have 2 pets',\n",
       " 'There are clothes everywhere',\n",
       " 'He lives in a big house',\n",
       " 'You have a red house',\n",
       " 'My favorite day is Friday')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybraille\n",
      "  Downloading pybraille-1.0.0.tar.gz (3.1 kB)\n",
      "Building wheels for collected packages: pybraille\n",
      "  Building wheel for pybraille (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pybraille: filename=pybraille-1.0.0-py3-none-any.whl size=3523 sha256=0a7f1dad0c23ea4d61b9f033f467d1c83b572ca2d887b7ec04e98c691b0e389b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/f0/da/95/9059ab7b8bd0cde8e16b8c25a601dc21100a65072e1d688684\n",
      "Successfully built pybraille\n",
      "Installing collected packages: pybraille\n",
      "Successfully installed pybraille-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pybraille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⠓⠑⠇⠇⠕'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pybraille import convertText\n",
    "\n",
    "convertText(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras import layers as L\n",
    "from keras.models import Model,load_model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 images belonging to 3 classes.\n",
      "Found 12 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory('./266 Photos/',\n",
    "                                              #target_size=(28,28),\n",
    "                                              subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory('./266 Photos/',\n",
    "                                            #target_size=(28,28),\n",
    "                                            subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convertText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f23ed097cf42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvertText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'22'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convertText' is not defined"
     ]
    }
   ],
   "source": [
    "convertText('22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model_ckpt = ModelCheckpoint('BrailleNet.h5',save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(patience=8,verbose=0)\n",
    "early_stop = EarlyStopping(patience=15,verbose=1)\n",
    "\n",
    "entry = L.Input(shape=(28,28,3))\n",
    "x = L.SeparableConv2D(64,(3,3),activation='relu')(entry)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.SeparableConv2D(128,(3,3),activation='relu')(x)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.SeparableConv2D(256,(2,2),activation='relu')(x)\n",
    "x = L.GlobalMaxPooling2D()(x)\n",
    "x = L.Dense(256)(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dense(64,kernel_regularizer=l2(2e-4))(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dense(26,activation='softmax')(x)\n",
    "\n",
    "model = Model(entry,x)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(train_generator,validation_data=val_generator,epochs=10,\n",
    "                              callbacks=[model_ckpt,reduce_lr,early_stop],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
