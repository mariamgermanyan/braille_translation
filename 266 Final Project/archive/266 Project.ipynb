{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras import layers as L\n",
    "from keras.models import Model,load_model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, T5Tokenizer, TFT5ForConditionalGeneration\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./266 Photos/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "\n",
    "    all_files = glob.glob(path + \"/*.png\")\n",
    "\n",
    "    sentences = []\n",
    "    images = []\n",
    "    \n",
    "    for filename in all_files:\n",
    "        img = Image.open(filename)\n",
    "        img = np.array(img) / 255.\n",
    "        images.append(img)\n",
    "        sentence = os.path.basename(os.path.normpath(os.path.splitext(filename)[0]))\n",
    "        sentences.append(sentence)\n",
    "\n",
    "\n",
    "    return sentences, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences, images = read_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = \"./266 Photos/validation\"\n",
    "english_sentences.extend(read_files(new_path)[0])\n",
    "images.extend(read_files(new_path[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc389c8afd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am happy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⠠⠊ ⠁⠍ ⠓⠁⠏⠏⠽', '⠙⠁⠽')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pybraille import convertText\n",
    "\n",
    "convertText(\"I am happy\"), convertText(\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "braille_sentences = []\n",
    "for sentence in english_sentences:\n",
    "    braille_sentences.append(convertText(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_english = []\n",
    "alphabet_braille = []\n",
    "alpha = 'a'\n",
    "for i in range(0, 26): \n",
    "    alphabet_english.append(alpha)\n",
    "    alphabet_braille.append(convertText(alpha))\n",
    "    alpha = chr(ord(alpha) + 1)\n",
    "for i in range(0, 10): \n",
    "    alphabet_english.append(i)\n",
    "    alphabet_braille.append(convertText(str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⠼⠁', '⠼⠃', '⠼⠁⠃')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertText('1'), convertText('2'), convertText('12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'happy']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(english_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 146, 1821, 2816, 102]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(english_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['⠠⠊', '⠁⠍', '⠓⠁⠏⠏⠽']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "braille_sentences[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 100, 100, 100, 102]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(braille_sentences[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', '[UNK]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(braille_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfef2664acd4e2a9f807926b88e4233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fddd7d8ca834a268f3ba18132570c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=526681800, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7fc36c216910>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "bert.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentences later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: english sentence e, foreign sentence f, hash of translation probabilities t, epsilon \n",
    "# Output: probability of e given f\n",
    "\n",
    "def probability_e_f(e, f, t, epsilon=1):\n",
    "    l_e = len(e)\n",
    "    l_f = len(f)\n",
    "    p_e_f = 1\n",
    "    \n",
    "    for ew in e: # iterate over english words ew in english sentence e\n",
    "        inner_sum = 0\n",
    "        for fw in f: # iterate over foreign words fw in foreign sentence f\n",
    "            inner_sum += t[(ew, fw)]\n",
    "        p_e_f = inner_sum * p_e_f\n",
    "    \n",
    "    p_e_f = p_e_f * epsilon / (l_f**l_e)\n",
    "    \n",
    "    return p_e_f  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'happy']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab:  ['100', '12', '2', '22', '3', 'a', 'Alice', 'am', 'America', 'an', 'and', 'Angeles', 'Anna', 'apples', 'are', 'ask', 'asparagus', 'at', 'attend', 'awhile', 'baby', 'basement', 'be', 'bed', 'been', 'behind', 'Berkeley', 'best', 'big', 'brussel', 'California', 'can', 'child', 'chips', 'Christina', 'cities', 'Claus', 'clothes', 'color', 'coming', 'cooking', 'couch', 'cream', 'cute', 'day', 'designated', 'dinner', 'divided', 'do', 'does', 'dog', 'dogs', 'doing', 'door', 'drink', 'driver', 'eat', 'eating', 'Every', 'everyday', 'everywhere', 'favor', 'favorite', 'fish', 'for', 'Friday', 'friend', 'friends', 'from', 'fuzzy', 'games', 'getting', 'Go', 'go', 'goes', 'going', 'good', 'groups', 'had', 'happy', 'has', 'have', 'He', 'her', 'here', 'Hi', 'high', 'his', 'homework', 'hospital', 'house', 'hungry', 'I', 'ice', 'in', 'into', 'is', 'John', 'laundry', 'left', 'library', 'like', 'likes', 'list', 'lives', 'Look', 'Los', 'love', 'loves', 'make', 'mall', 'Mariam', 'Mary', 'museum', 'my', 'My', 'name', 'need', 'old', 'older', 'on', 'open', 'Oysters', 'pets', 'pink', 'play', 'playing', 'proposed', 'red', 'refuge', 'Santa', 'saw', 'school', 'She', 'shirt', 'sister', 'sky', 'slimy', 'slippers', 'smoke', 'sprouts', 'store', 'study', 'tacos', 'The', 'the', 'There', 'These', 'They', 'thirsty', 'This', 'this', 'Thursday', 'to', 'Today', 'tonight', 'took', 'town', 'tree', 'UC', 'vacation', 'very', 'video', 'want', 'wanting', 'was', 'We', 'wears', 'went', 'What', 'when', 'will', 'work', 'years', 'You', 'you', 'your']\n",
      "Foreign vocab:  ['⠁', '⠁⠍', '⠁⠎⠅', '⠁⠎⠏⠁⠗⠁⠛⠥⠎', '⠁⠏⠏⠇⠑⠎', '⠁⠗⠑', '⠁⠝', '⠁⠝⠙', '⠁⠞', '⠁⠞⠞⠑⠝⠙', '⠁⠺⠓⠊⠇⠑', '⠃⠁⠃⠽', '⠃⠁⠎⠑⠍⠑⠝⠞', '⠃⠊⠛', '⠃⠑', '⠃⠑⠎⠞', '⠃⠑⠑⠝', '⠃⠑⠓⠊⠝⠙', '⠃⠑⠙', '⠃⠗⠥⠎⠎⠑⠇', '⠇⠁⠥⠝⠙⠗⠽', '⠇⠊⠃⠗⠁⠗⠽', '⠇⠊⠅⠑', '⠇⠊⠅⠑⠎', '⠇⠊⠎⠞', '⠇⠊⠧⠑⠎', '⠇⠑⠋⠞', '⠇⠕⠧⠑', '⠇⠕⠧⠑⠎', '⠉⠁⠝', '⠉⠇⠕⠞⠓⠑⠎', '⠉⠊⠞⠊⠑⠎', '⠉⠓⠊⠇⠙', '⠉⠓⠊⠏⠎', '⠉⠕⠇⠕⠗', '⠉⠕⠍⠊⠝⠛', '⠉⠕⠕⠅⠊⠝⠛', '⠉⠕⠥⠉⠓', '⠉⠗⠑⠁⠍', '⠉⠥⠞⠑', '⠊⠉⠑', '⠊⠎', '⠊⠝', '⠊⠝⠞⠕', '⠋⠁⠧⠕⠗', '⠋⠁⠧⠕⠗⠊⠞⠑', '⠋⠊⠎⠓', '⠋⠕⠗', '⠋⠗⠊⠑⠝⠙', '⠋⠗⠊⠑⠝⠙⠎', '⠋⠗⠕⠍', '⠋⠥⠵⠵⠽', '⠍⠁⠅⠑', '⠍⠁⠇⠇', '⠍⠥⠎⠑⠥⠍', '⠍⠽', '⠎⠁⠺', '⠎⠅⠽', '⠎⠇⠊⠍⠽', '⠎⠇⠊⠏⠏⠑⠗⠎', '⠎⠉⠓⠕⠕⠇', '⠎⠊⠎⠞⠑⠗', '⠎⠍⠕⠅⠑', '⠎⠏⠗⠕⠥⠞⠎', '⠎⠓⠊⠗⠞', '⠎⠞⠕⠗⠑', '⠎⠞⠥⠙⠽', '⠏⠇⠁⠽', '⠏⠇⠁⠽⠊⠝⠛', '⠏⠊⠝⠅', '⠏⠑⠞⠎', '⠏⠗⠕⠏⠕⠎⠑⠙', '⠑⠁⠞', '⠑⠁⠞⠊⠝⠛', '⠑⠧⠑⠗⠽⠙⠁⠽', '⠑⠧⠑⠗⠽⠺⠓⠑⠗⠑', '⠓⠁⠎', '⠓⠁⠏⠏⠽', '⠓⠁⠙', '⠓⠁⠧⠑', '⠓⠊⠎', '⠓⠊⠛⠓', '⠓⠑⠗', '⠓⠑⠗⠑', '⠓⠕⠍⠑⠺⠕⠗⠅', '⠓⠕⠎⠏⠊⠞⠁⠇', '⠓⠕⠥⠎⠑', '⠓⠥⠝⠛⠗⠽', '⠕⠇⠙', '⠕⠇⠙⠑⠗', '⠕⠏⠑⠝', '⠕⠝', '⠗⠑⠋⠥⠛⠑', '⠗⠑⠙', '⠙⠁⠽', '⠙⠊⠝⠝⠑⠗', '⠙⠊⠧⠊⠙⠑⠙', '⠙⠑⠎⠊⠛⠝⠁⠞⠑⠙', '⠙⠕', '⠙⠕⠊⠝⠛', '⠙⠕⠑⠎', '⠙⠕⠕⠗', '⠙⠕⠛', '⠙⠕⠛⠎', '⠙⠗⠊⠝⠅', '⠙⠗⠊⠧⠑⠗', '⠛⠁⠍⠑⠎', '⠛⠑⠞⠞⠊⠝⠛', '⠛⠕', '⠛⠕⠊⠝⠛', '⠛⠕⠑⠎', '⠛⠕⠕⠙', '⠛⠗⠕⠥⠏⠎', '⠝⠁⠍⠑', '⠝⠑⠑⠙', '⠞⠁⠉⠕⠎', '⠞⠓⠊⠎', '⠞⠓⠊⠗⠎⠞⠽', '⠞⠓⠑', '⠞⠕', '⠞⠕⠕⠅', '⠞⠕⠝⠊⠛⠓⠞', '⠞⠕⠺⠝', '⠞⠗⠑⠑', '⠠⠁⠇⠊⠉⠑', '⠠⠁⠍⠑⠗⠊⠉⠁', '⠠⠁⠝⠛⠑⠇⠑⠎', '⠠⠁⠝⠝⠁', '⠠⠃⠑⠗⠅⠑⠇⠑⠽', '⠠⠇⠕⠎', '⠠⠇⠕⠕⠅', '⠠⠉⠁⠇⠊⠋⠕⠗⠝⠊⠁', '⠠⠉⠇⠁⠥⠎', '⠠⠉⠓⠗⠊⠎⠞⠊⠝⠁', '⠠⠊', '⠠⠋⠗⠊⠙⠁⠽', '⠠⠍⠁⠗⠊⠁⠍', '⠠⠍⠁⠗⠽', '⠠⠍⠽', '⠠⠎⠁⠝⠞⠁', '⠠⠎⠓⠑', '⠠⠑⠧⠑⠗⠽', '⠠⠓⠊', '⠠⠓⠑', '⠠⠕⠽⠎⠞⠑⠗⠎', '⠠⠚⠕⠓⠝', '⠠⠛⠕', '⠠⠞⠓⠊⠎', '⠠⠞⠓⠑', '⠠⠞⠓⠑⠎⠑', '⠠⠞⠓⠑⠗⠑', '⠠⠞⠓⠑⠽', '⠠⠞⠓⠥⠗⠎⠙⠁⠽', '⠠⠞⠕⠙⠁⠽', '⠠⠥⠠⠉', '⠠⠺⠑', '⠠⠺⠓⠁⠞', '⠠⠽⠕⠥', '⠧⠁⠉⠁⠞⠊⠕⠝', '⠧⠊⠙⠑⠕', '⠧⠑⠗⠽', '⠺⠁⠎', '⠺⠁⠝⠞', '⠺⠁⠝⠞⠊⠝⠛', '⠺⠊⠇⠇', '⠺⠑⠁⠗⠎', '⠺⠑⠝⠞', '⠺⠓⠑⠝', '⠺⠕⠗⠅', '⠼⠁⠃', '⠼⠁⠚⠚', '⠼⠃', '⠼⠃⠃', '⠼⠉', '⠽⠑⠁⠗⠎', '⠽⠕⠥', '⠽⠕⠥⠗']\n",
      "english_vocab_size:  177\n",
      "foreign_vocab_size:  177\n"
     ]
    }
   ],
   "source": [
    "# Extract foreign and english vocabularies\n",
    "foreign_words = []\n",
    "english_words = []\n",
    "\n",
    "for index in range(len(english_sentences)):\n",
    "    for word in english_sentences[index].split(): \n",
    "        english_words.append(word)\n",
    "    for word in braille_sentences[index].split(): \n",
    "        foreign_words.append(word)\n",
    "        \n",
    "english_words = sorted(list(set(english_words)), key=lambda s: s.lower()) \n",
    "foreign_words = sorted(list(set(foreign_words)), key=lambda s: s.lower())\n",
    "print('English vocab: ', english_words)\n",
    "print('Foreign vocab: ', foreign_words)\n",
    "\n",
    "english_vocab_size = len(english_words)\n",
    "foreign_vocab_size = len(foreign_words)\n",
    "print('english_vocab_size: ', english_vocab_size)\n",
    "print('foreign_vocab_size: ', foreign_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to uniformly initialize word translation probabilities in t hash\n",
    "\n",
    "def init_prob(t, init_val, english_words, foreign_words):\n",
    "    for fw in foreign_words:\n",
    "        for ew in english_words:\n",
    "            tup = (ew, fw) # tuple required because dict key cannot be list\n",
    "            t[tup] = init_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Collection of sentence pairs sentence_pairs, hash of translation probabilities t, epsilon\n",
    "# Output: Perplexity of model\n",
    "\n",
    "def perplexity(sentence_pairs, t, epsilon=1, debug_output=False):\n",
    "    pp = 0\n",
    "    \n",
    "    for sp in sentence_pairs:\n",
    "        prob = probability_e_f(sp[1], sp[0], t)\n",
    "        if debug_output:\n",
    "            print('english sentence:', sp[1], 'foreign sentence:', sp[0])\n",
    "            print(prob)\n",
    "            print()\n",
    "        pp += math.log(prob, 2) # log base 2\n",
    "        \n",
    "    pp = 2.0**(-pp)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs = [ \n",
    "    [ ['⠞⠓⠑', '⠠⠓⠁⠥⠎'], ['the', 'house'] ], \n",
    "    [ ['⠞⠓⠑', '⠃⠕⠕⠅'], ['the', 'book'] ], \n",
    "    [ ['⠁', '⠃⠕⠕⠅'], ['a', 'book'] ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '12', '2', '22', '3', 'a', 'Alice', 'am', 'America', 'an', 'and', 'Angeles', 'Anna', 'apples', 'are', 'ask', 'asparagus', 'at', 'attend', 'awhile', 'baby', 'basement', 'be', 'bed', 'been', 'behind', 'Berkeley', 'best', 'big', 'brussel', 'California', 'can', 'child', 'chips', 'Christina', 'cities', 'Claus', 'clothes', 'color', 'coming', 'cooking', 'couch', 'cream', 'cute', 'day', 'designated', 'dinner', 'divided', 'do', 'does', 'dog', 'dogs', 'doing', 'door', 'drink', 'driver', 'eat', 'eating', 'Every', 'everyday', 'everywhere', 'favor', 'favorite', 'fish', 'for', 'Friday', 'friend', 'friends', 'from', 'fuzzy', 'games', 'getting', 'Go', 'go', 'goes', 'going', 'good', 'groups', 'had', 'happy', 'has', 'have', 'He', 'her', 'here', 'Hi', 'high', 'his', 'homework', 'hospital', 'house', 'hungry', 'I', 'ice', 'in', 'into', 'is', 'John', 'laundry', 'left', 'library', 'like', 'likes', 'list', 'lives', 'Look', 'Los', 'love', 'loves', 'make', 'mall', 'Mariam', 'Mary', 'museum', 'my', 'My', 'name', 'need', 'old', 'older', 'on', 'open', 'Oysters', 'pets', 'pink', 'play', 'playing', 'proposed', 'red', 'refuge', 'Santa', 'saw', 'school', 'She', 'shirt', 'sister', 'sky', 'slimy', 'slippers', 'smoke', 'sprouts', 'store', 'study', 'tacos', 'The', 'the', 'There', 'These', 'They', 'thirsty', 'This', 'this', 'Thursday', 'to', 'Today', 'tonight', 'took', 'town', 'tree', 'UC', 'vacation', 'very', 'video', 'want', 'wanting', 'was', 'We', 'wears', 'went', 'What', 'when', 'will', 'work', 'years', 'You', 'you', 'your']\n",
      "['⠁', '⠁⠍', '⠁⠎⠅', '⠁⠎⠏⠁⠗⠁⠛⠥⠎', '⠁⠏⠏⠇⠑⠎', '⠁⠗⠑', '⠁⠝', '⠁⠝⠙', '⠁⠞', '⠁⠞⠞⠑⠝⠙', '⠁⠺⠓⠊⠇⠑', '⠃⠁⠃⠽', '⠃⠁⠎⠑⠍⠑⠝⠞', '⠃⠊⠛', '⠃⠑', '⠃⠑⠎⠞', '⠃⠑⠑⠝', '⠃⠑⠓⠊⠝⠙', '⠃⠑⠙', '⠃⠗⠥⠎⠎⠑⠇', '⠇⠁⠥⠝⠙⠗⠽', '⠇⠊⠃⠗⠁⠗⠽', '⠇⠊⠅⠑', '⠇⠊⠅⠑⠎', '⠇⠊⠎⠞', '⠇⠊⠧⠑⠎', '⠇⠑⠋⠞', '⠇⠕⠧⠑', '⠇⠕⠧⠑⠎', '⠉⠁⠝', '⠉⠇⠕⠞⠓⠑⠎', '⠉⠊⠞⠊⠑⠎', '⠉⠓⠊⠇⠙', '⠉⠓⠊⠏⠎', '⠉⠕⠇⠕⠗', '⠉⠕⠍⠊⠝⠛', '⠉⠕⠕⠅⠊⠝⠛', '⠉⠕⠥⠉⠓', '⠉⠗⠑⠁⠍', '⠉⠥⠞⠑', '⠊⠉⠑', '⠊⠎', '⠊⠝', '⠊⠝⠞⠕', '⠋⠁⠧⠕⠗', '⠋⠁⠧⠕⠗⠊⠞⠑', '⠋⠊⠎⠓', '⠋⠕⠗', '⠋⠗⠊⠑⠝⠙', '⠋⠗⠊⠑⠝⠙⠎', '⠋⠗⠕⠍', '⠋⠥⠵⠵⠽', '⠍⠁⠅⠑', '⠍⠁⠇⠇', '⠍⠥⠎⠑⠥⠍', '⠍⠽', '⠎⠁⠺', '⠎⠅⠽', '⠎⠇⠊⠍⠽', '⠎⠇⠊⠏⠏⠑⠗⠎', '⠎⠉⠓⠕⠕⠇', '⠎⠊⠎⠞⠑⠗', '⠎⠍⠕⠅⠑', '⠎⠏⠗⠕⠥⠞⠎', '⠎⠓⠊⠗⠞', '⠎⠞⠕⠗⠑', '⠎⠞⠥⠙⠽', '⠏⠇⠁⠽', '⠏⠇⠁⠽⠊⠝⠛', '⠏⠊⠝⠅', '⠏⠑⠞⠎', '⠏⠗⠕⠏⠕⠎⠑⠙', '⠑⠁⠞', '⠑⠁⠞⠊⠝⠛', '⠑⠧⠑⠗⠽⠙⠁⠽', '⠑⠧⠑⠗⠽⠺⠓⠑⠗⠑', '⠓⠁⠎', '⠓⠁⠏⠏⠽', '⠓⠁⠙', '⠓⠁⠧⠑', '⠓⠊⠎', '⠓⠊⠛⠓', '⠓⠑⠗', '⠓⠑⠗⠑', '⠓⠕⠍⠑⠺⠕⠗⠅', '⠓⠕⠎⠏⠊⠞⠁⠇', '⠓⠕⠥⠎⠑', '⠓⠥⠝⠛⠗⠽', '⠕⠇⠙', '⠕⠇⠙⠑⠗', '⠕⠏⠑⠝', '⠕⠝', '⠗⠑⠋⠥⠛⠑', '⠗⠑⠙', '⠙⠁⠽', '⠙⠊⠝⠝⠑⠗', '⠙⠊⠧⠊⠙⠑⠙', '⠙⠑⠎⠊⠛⠝⠁⠞⠑⠙', '⠙⠕', '⠙⠕⠊⠝⠛', '⠙⠕⠑⠎', '⠙⠕⠕⠗', '⠙⠕⠛', '⠙⠕⠛⠎', '⠙⠗⠊⠝⠅', '⠙⠗⠊⠧⠑⠗', '⠛⠁⠍⠑⠎', '⠛⠑⠞⠞⠊⠝⠛', '⠛⠕', '⠛⠕⠊⠝⠛', '⠛⠕⠑⠎', '⠛⠕⠕⠙', '⠛⠗⠕⠥⠏⠎', '⠝⠁⠍⠑', '⠝⠑⠑⠙', '⠞⠁⠉⠕⠎', '⠞⠓⠊⠎', '⠞⠓⠊⠗⠎⠞⠽', '⠞⠓⠑', '⠞⠕', '⠞⠕⠕⠅', '⠞⠕⠝⠊⠛⠓⠞', '⠞⠕⠺⠝', '⠞⠗⠑⠑', '⠠⠁⠇⠊⠉⠑', '⠠⠁⠍⠑⠗⠊⠉⠁', '⠠⠁⠝⠛⠑⠇⠑⠎', '⠠⠁⠝⠝⠁', '⠠⠃⠑⠗⠅⠑⠇⠑⠽', '⠠⠇⠕⠎', '⠠⠇⠕⠕⠅', '⠠⠉⠁⠇⠊⠋⠕⠗⠝⠊⠁', '⠠⠉⠇⠁⠥⠎', '⠠⠉⠓⠗⠊⠎⠞⠊⠝⠁', '⠠⠊', '⠠⠋⠗⠊⠙⠁⠽', '⠠⠍⠁⠗⠊⠁⠍', '⠠⠍⠁⠗⠽', '⠠⠍⠽', '⠠⠎⠁⠝⠞⠁', '⠠⠎⠓⠑', '⠠⠑⠧⠑⠗⠽', '⠠⠓⠊', '⠠⠓⠑', '⠠⠕⠽⠎⠞⠑⠗⠎', '⠠⠚⠕⠓⠝', '⠠⠛⠕', '⠠⠞⠓⠊⠎', '⠠⠞⠓⠑', '⠠⠞⠓⠑⠎⠑', '⠠⠞⠓⠑⠗⠑', '⠠⠞⠓⠑⠽', '⠠⠞⠓⠥⠗⠎⠙⠁⠽', '⠠⠞⠕⠙⠁⠽', '⠠⠥⠠⠉', '⠠⠺⠑', '⠠⠺⠓⠁⠞', '⠠⠽⠕⠥', '⠧⠁⠉⠁⠞⠊⠕⠝', '⠧⠊⠙⠑⠕', '⠧⠑⠗⠽', '⠺⠁⠎', '⠺⠁⠝⠞', '⠺⠁⠝⠞⠊⠝⠛', '⠺⠊⠇⠇', '⠺⠑⠁⠗⠎', '⠺⠑⠝⠞', '⠺⠓⠑⠝', '⠺⠕⠗⠅', '⠼⠁⠃', '⠼⠁⠚⠚', '⠼⠃', '⠼⠃⠃', '⠼⠉', '⠽⠑⠁⠗⠎', '⠽⠕⠥', '⠽⠕⠥⠗']\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 5\n",
    "perplex = []\n",
    "debug_output = True\n",
    "s_total = {}\n",
    "\n",
    "# Initialize probabilities uniformly\n",
    "t = {}\n",
    "init_val = 1.0 / foreign_vocab_size\n",
    "init_prob(t, init_val, english_words, foreign_words)\n",
    "print(english_words)\n",
    "print(foreign_words);\n",
    "\n",
    "# if debug_output:\n",
    "#     print('Hash initialized')\n",
    "#     print('No. of foreign/english pairs: ', len(t))\n",
    "#     print('Content: ', t)\n",
    "#     print('**************')\n",
    "#     print()\n",
    "\n",
    "# # Loop while not converged\n",
    "# for iter in range(num_iterations):\n",
    "    \n",
    "#     # Calculate perplexity\n",
    "#     pp = perplexity(sentence_pairs, t, 1, True)\n",
    "#     print(pp)\n",
    "#     print('**************')\n",
    "#     perplex.append(pp)\n",
    "\n",
    "#     # Initialize\n",
    "#     count = {}\n",
    "#     total = {}\n",
    "\n",
    "#     for fw in foreign_words:\n",
    "#         total[fw] = 0.0\n",
    "#         for ew in english_words:\n",
    "#             count[(ew, fw)] = 0.0\n",
    "\n",
    "#     for sp in sentence_pairs:\n",
    "\n",
    "#         # Compute normalization\n",
    "#         for ew in sp[1]:\n",
    "#             s_total[ew] = 0.0\n",
    "#             for fw in sp[0]:\n",
    "#                 s_total[ew] += t[(ew, fw)]\n",
    "\n",
    "#         # Collect counts\n",
    "#         for ew in sp[1]:\n",
    "#             for fw in sp[0]:\n",
    "#                 count[(ew, fw)] += t[(ew, fw)] / s_total[ew]\n",
    "#                 total[fw] += t[(ew, fw)] / s_total[ew]\n",
    "\n",
    "#     # Estimate probabilities\n",
    "#     for fw in foreign_words:\n",
    "#         for ew in english_words:\n",
    "#             t[(ew, fw)] = count[(ew, fw)] / total[fw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./266 Photos/training\"\n",
    "# all_files = glob.glob(path + \"/*.png\")\n",
    "# for filename in all_files:\n",
    "#     os.mkdir(filename[:-4])\n",
    "    \n",
    "# path = \"./266 Photos/validation\"\n",
    "# all_files = glob.glob(path + \"/*.png\")\n",
    "# for filename in all_files:\n",
    "#     os.mkdir(filename[:-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator()\n",
    "\n",
    "# train_generator = datagen.flow_from_directory('./266 Photos/training')\n",
    "\n",
    "# val_generator = datagen.flow_from_directory('./266 Photos/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "\n",
    "# model_ckpt = ModelCheckpoint('BrailleNet.h5',save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(patience=8,verbose=0)\n",
    "# early_stop = EarlyStopping(patience=15,verbose=1)\n",
    "\n",
    "# entry = L.Input(shape=(28,28,3))\n",
    "# x = L.SeparableConv2D(64,(3,3),activation='relu')(entry)\n",
    "# x = L.MaxPooling2D((2,2))(x)\n",
    "# x = L.SeparableConv2D(128,(3,3),activation='relu')(x)\n",
    "# x = L.MaxPooling2D((2,2))(x)\n",
    "# x = L.SeparableConv2D(256,(2,2),activation='relu')(x)\n",
    "# x = L.GlobalMaxPooling2D()(x)\n",
    "# x = L.Dense(256)(x)\n",
    "# x = L.LeakyReLU()(x)\n",
    "# x = L.Dense(64,kernel_regularizer=l2(2e-4))(x)\n",
    "# x = L.LeakyReLU()(x)\n",
    "# x = L.Dense(26,activation='softmax')(x)\n",
    "\n",
    "# model = Model(entry,x)\n",
    "# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_generator,validation_data=val_generator,epochs=10,\n",
    "#                               callbacks=[model_ckpt,reduce_lr,early_stop],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
